import pandas as pd
import numpy as np
import lightgbm as lgb
from scipy.sparse import vstack, csr_matrix, save_npz, load_npz
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
import gc  # garbage collector

from constants import remove_cols, dtypes

gc.enable()


# reference for data preprocess:
# https://www.kaggle.com/code/jiegeng94/everyone-do-this-at-the-beginning
# https://github.com/Diyago/ML-DL-scripts
def preprocess(train_file='../Source/train_sample.csv', test_file='../Source/test_sample.csv', column_lower_limit=10):

    print('\nRead Train and Test Data for light gbm\n')
    train = pd.read_csv(train_file, dtype=dtypes, low_memory=True)
    test = pd.read_csv(test_file, dtype=dtypes, low_memory=True)

    # remove unnecessary lines
    # 1.Mostly-missing features
    # 2.Too-skewed features
    # 3.Highly-correlated features
    train.drop(remove_cols, axis=1, inplace=True)
    test.drop(remove_cols, axis=1, inplace=True)

    # Reset the MachineIdentifier as index
    train['MachineIdentifier'] = train.index.astype('uint32')
    test['MachineIdentifier'] = test.index.astype('uint32')

    gc.collect()

    print('Transform all features to values.\n')
    for column in train.columns.tolist()[1:-1]:
        # First set the type as string
        train[column] = train[column].astype('str')
        test[column] = test[column].astype('str')

        # Fit LabelEncoder
        label_encoder = LabelEncoder().fit(
            np.unique(train[column].unique().tolist() +
                      test[column].unique().tolist())
        )

        # The start value is 1, 0 will be used for dropped values
        train[column] = label_encoder.transform(train[column]) + 1
        test[column] = label_encoder.transform(test[column]) + 1

        # Group the data by the curren column to
        # 1. remove observations less than the lower limit
        # 2. remove unbalanced values between training set and test set
        aggregate_train = (train
                           .groupby([column])
                           .aggregate({'MachineIdentifier': 'count'})
                           .reset_index()
                           .rename({'MachineIdentifier': 'Train'}, axis=1))
        aggregate_test = (test
                          .groupby([column])
                          .aggregate({'MachineIdentifier': 'count'})
                          .reset_index()
                          .rename({'MachineIdentifier': 'Test'}, axis=1))

        aggregate = pd.merge(aggregate_train, aggregate_test, on=column, how='outer').replace(np.nan, 0)

        # Select values the number of which in training set is higher than the lower limit
        aggregate = aggregate[(aggregate['Train'] > column_lower_limit)].reset_index(drop=True)
        # Get the total number of a specific value
        aggregate['Total'] = aggregate['Train'] + aggregate['Test']

        # Drop unbalanced values
        # The ratio of the number in training set and test set is between 0.2 and 0.8
        aggregate = aggregate[
            (aggregate['Train'] / aggregate['Total'] > 0.2) & (aggregate['Train'] / aggregate['Total'] < 0.8)]
        aggregate[column + '_copy'] = aggregate[column]

        # Only keep balanced values with number more than lower limit
        # For other values, replace them to 0
        train[column] = (pd.merge(train[[column]],
                                  aggregate[[column, column + '_copy']],
                                  on=column, how='left')[column + '_copy']
                         .replace(np.nan, 0).astype('int').astype('category'))

        test[column] = (pd.merge(test[[column]],
                                 aggregate[[column, column + '_copy']],
                                 on=column, how='left')[column + '_copy']
                        .replace(np.nan, 0).astype('int').astype('category'))

        del label_encoder, aggregate_train, aggregate_test, aggregate, column
        gc.collect()

    y_train = np.array(train['HasDetections'])
    train_ids = train.index
    test_ids = test.index

    del train['HasDetections'], train['MachineIdentifier'], test['MachineIdentifier']
    gc.collect()

    # Fit OneHotEncoder
    ohe = OneHotEncoder(categories='auto', sparse_output=True, dtype='uint8', handle_unknown='ignore').fit(train)

    # Transform data using small groups to reduce memory usage
    cnt = 100000
    train = vstack([ohe.transform(train[i * cnt:(i + 1) * cnt]) for i in range(train.shape[0] // cnt + 1)])
    test = vstack([ohe.transform(test[i * cnt:(i + 1) * cnt]) for i in range(test.shape[0] // cnt + 1)])
    save_npz('../Source/train.npz', train, compressed=True)
    save_npz('../Source/test.npz', test, compressed=True)

    del ohe
    gc.collect()

    return train_ids, test_ids, y_train


def light_gbm(is_sample=True):

    if is_sample:
        train_ids, test_ids, y_train = preprocess()
    else:
        train_ids, test_ids, y_train = preprocess(train_file='../Source/train.csv', test_file='../Source/test.csv',
                                                  column_lower_limit=1000)

    skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)
    skf.get_n_splits(train_ids, y_train)

    lgb_test_result = np.zeros(test_ids.shape[0])
    counter = 0

    print('\nLightGBM\n')
    cnt = 100000
    for train_index, test_index in skf.split(train_ids, y_train):
        print('Fold {}'.format(counter + 1))

        train = load_npz('../Source/train.npz')
        x_fit = vstack([train[train_index[i * cnt:(i + 1) * cnt]] for i in range(train_index.shape[0] // cnt + 1)])
        x_val = vstack([train[test_index[i * cnt:(i + 1) * cnt]] for i in range(test_index.shape[0] // cnt + 1)])
        x_fit, x_val = csr_matrix(x_fit, dtype='float32'), csr_matrix(x_val, dtype='float32')
        y_fit, y_val = y_train[train_index], y_train[test_index]

        del train
        gc.collect()

        lgb_model = lgb.LGBMClassifier(
            n_estimators=5000,
            min_split_gain=0.0001,
            min_child_weight=25,
            num_leaves=300,
            min_data_in_leaf=125,
            objective='binary',
            silent=-1,
            n_jobs=-1,
            learning_rate=0.05,
            boosting_type="gbdt",
            feature_fraction=0.5,
            bagging_fraction=0.901,
            max_depth=14,
            reg_alpha=2.5,
            reg_lambda=2.5,
            random_state=133,
            verbosity=-1
        )

        lgb_model.fit(x_fit, y_fit, eval_metric='auc', eval_set=[(x_val, y_val)])

        pred = lgb_model.predict(x_val)
        num = accuracy_score(y_val, pred)
        print("Testing accuracy {:.4f}\n".format(num))

        del x_fit, x_val, y_fit, y_val, train_index, test_index
        gc.collect()

        test = load_npz('../Source/test.npz')
        test = csr_matrix(test, dtype='float32')
        lgb_test_result += lgb_model.predict_proba(test)[:, 1]
        counter += 1

        del test
        gc.collect()

    result = pd.read_csv('../Source/test_sample.csv')
    result['HasDetections'] = lgb_test_result / counter
    result.to_csv('../Source/test_sample_lgbm.csv', index=False)


if __name__ == '__main__':
    light_gbm()
