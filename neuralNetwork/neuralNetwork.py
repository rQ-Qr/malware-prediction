import math

import pandas as pd
import torch
import numpy as np
from torch import nn
from torch.autograd import Variable
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import gc  # garbage collector
from constants import dtypes, remove_cols
from constants import frequencyEncoding, oneHotEncoding


def frequency_encoder(df, cols):
    for col in cols:
        counts = df[col].value_counts(dropna=False)
        df[col] = df[col].map(counts).astype('int') / counts.max()


def one_hot_encoder(df, col):
    maps = df[col].value_counts(dropna=False)

    n = []
    num = 0
    d = {}
    for idx in maps.index:
        new_col = col + '_OHE_' + str(idx)
        if isinstance(idx, float) and math.isnan(idx):
            df[new_col] = (df[col].isna()).astype('int8')
        else:
            df[new_col] = (df[col] == idx).astype('int8')
        n.append(new_col)
        d[idx] = 1
        num += 1
        if (num + 1) >= len(maps):
            break

    return [n, d]


def one_hot_encoder_for_test(df, col, maps):
    cols = []
    for value in maps:
        new_col = col + '_OHE_' + str(value)
        if isinstance(value, float) and math.isnan(value):
            df[new_col] = df[col].isna().astype('int8')
        else:
            df[new_col] = (df[col] == value).astype('int8')
        cols += [new_col]

    return cols


def preprocess(train_file='../Source/train_sample.csv', test_file='../Source/test_sample.csv', column_lower_limit=10):

    print('Read Train and Test Data.\n')
    train = pd.read_csv(train_file, dtype=dtypes, low_memory=True)
    test = pd.read_csv(test_file, dtype=dtypes, low_memory=True)

    # remove unnecessary lines
    # 1.Mostly-missing features
    # 2.Too-skewed features
    # 3.Highly-correlated features
    train.drop(remove_cols, axis=1, inplace=True)
    test.drop(remove_cols, axis=1, inplace=True)

    # Reset the MachineIdentifier as index
    train['MachineIdentifier'] = train.index.astype('uint32')
    test['MachineIdentifier'] = test.index.astype('uint32')

    gc.collect()

    print('Transform all features to values.\n')
    for column in train.columns.tolist()[1:-1]:
        # First set the type as string
        train[column] = train[column].astype('str')
        test[column] = test[column].astype('str')

        # Fit LabelEncoder
        label_encoder = LabelEncoder().fit(
            np.unique(train[column].unique().tolist() +
                      test[column].unique().tolist())
        )

        # The start value is 1, 0 will be used for dropped values
        train[column] = label_encoder.transform(train[column]) + 1
        test[column] = label_encoder.transform(test[column]) + 1

        # Group the data by the curren column to
        # 1. remove observations less than the lower limit
        # 2. remove unbalanced values between training set and test set
        aggregate_train = (train
                           .groupby([column])
                           .aggregate({'MachineIdentifier': 'count'})
                           .reset_index()
                           .rename({'MachineIdentifier': 'Train'}, axis=1))
        aggregate_test = (test
                          .groupby([column])
                          .aggregate({'MachineIdentifier': 'count'})
                          .reset_index()
                          .rename({'MachineIdentifier': 'Test'}, axis=1))

        aggregate = pd.merge(aggregate_train, aggregate_test, on=column, how='outer').replace(np.nan, 0)

        # Select values the number of which in training set is higher than the lower limit
        aggregate = aggregate[(aggregate['Train'] > column_lower_limit)].reset_index(drop=True)
        # Get the total number of a specific value
        aggregate['Total'] = aggregate['Train'] + aggregate['Test']

        # Drop unbalanced values
        # The ratio of the number in training set and test set is between 0.2 and 0.8
        aggregate = aggregate[
            (aggregate['Train'] / aggregate['Total'] > 0.2) & (aggregate['Train'] / aggregate['Total'] < 0.8)]
        aggregate[column + '_copy'] = aggregate[column]

        # Only keep balanced values with number more than lower limit
        # For other values, replace them to 0
        train[column] = (pd.merge(train[[column]],
                                  aggregate[[column, column + '_copy']],
                                  on=column, how='left')[column + '_copy']
                         .replace(np.nan, 0).astype('int').astype('category'))

        test[column] = (pd.merge(test[[column]],
                                 aggregate[[column, column + '_copy']],
                                 on=column, how='left')[column + '_copy']
                        .replace(np.nan, 0).astype('int').astype('category'))

        del label_encoder, aggregate_train, aggregate_test, aggregate, column
        gc.collect()

    del train['MachineIdentifier'], test['MachineIdentifier']
    gc.collect()

    # Fit FrequencyEncoder
    frequency_encoder(train, frequencyEncoding)
    frequency_encoder(test, frequencyEncoding)

    # Fit OneHotEncoder for training set
    cols = frequencyEncoding
    maps = []
    # ENCODE NEW
    for col in oneHotEncoding:
        tmp = one_hot_encoder(train, col)
        cols += tmp[0]
        maps.append(tmp[1])
    print('Encoded', len(cols), 'new variables')

    # REMOVE OLD
    for col in oneHotEncoding:
        del train[col]
    print('Removed original', len(frequencyEncoding + oneHotEncoding), 'variables')
    gc.collect()

    # Fit OneHotEncoder for test set
    test_cols = frequencyEncoding
    for x in range(len(oneHotEncoding)):
        test_cols += one_hot_encoder_for_test(test, oneHotEncoding[x], maps[x])

    gc.collect()

    return train, test, cols, test_cols


def neural_network(is_sample=True):
    if is_sample:
        train, test, cols, test_cols = preprocess()
    else:
        train, test, cols, test_cols = preprocess(train_file='../Source/train.csv', test_file='../Source/test.csv',
                                                  column_lower_limit=1000)

    # SPLIT TRAIN AND VALIDATION SET
    x_fit, x_val, y_fit, y_val = train_test_split(train[cols], train['HasDetections'], test_size=0.3)

    print('\nNeural Network\n')

    del train
    gc.collect()

    dimension = len(cols)
    print('column dimension: {}\n'.format(dimension))

    # define the parameters
    net = nn.Sequential(
        nn.Linear(dimension, dimension // 4),
        nn.Tanh(),
        nn.Linear(dimension//4, dimension//64),
        nn.Tanh(),
        nn.Linear(dimension//64, dimension//512),
        nn.Tanh(),
        nn.Linear(dimension // 512, 1)
    )

    optimizer = torch.optim.SGD(net.parameters(), 0.1)

    criterion = nn.BCEWithLogitsLoss()

    for e in range(1000):
        out = net(Variable(torch.from_numpy(x_fit.astype('float32').to_numpy())))
        loss = criterion(out, Variable(torch.from_numpy(np.array(y_fit).astype('float'))).unsqueeze(1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (e + 1) % 200 == 0:
            print('epoch: {}, loss: {}'.format(e + 1, loss.data.item()))

    pred = F.sigmoid(net(Variable(torch.from_numpy(x_val.astype('float32').to_numpy()))))
    pred = (pred > 0.5) * 1
    num = accuracy_score(torch.from_numpy(np.array(y_val.astype('float32'))), pred)
    print("Testing accuracy {:.4f}".format(num))

    del pred, x_fit, x_val, y_fit, y_val
    gc.collect()

    pred = F.sigmoid(net(Variable(torch.from_numpy(test[test_cols].astype('float32').to_numpy()))))
    test_result = pred[:, 0].tolist()

    del test
    gc.collect()

    result = pd.read_csv('../Source/test_sample.csv')
    result['HasDetections'] = test_result
    result.to_csv('../Source/test_sample_neural.csv', index=False)

    print('end')


def neural_network_test():
    train, test, cols, test_cols = preprocess()

    # SPLIT TRAIN AND VALIDATION SET
    x_fit, x_val, y_fit, y_val = train_test_split(train[cols], train['HasDetections'], test_size=0.3)

    print('\nNeural Network with manual model\n')

    del train
    gc.collect()

    dimension = len(cols)
    print('column dimension: {}\n'.format(dimension))

    # define the parameters
    net = nn.Sequential(
        nn.Linear(dimension, dimension // 512),
        nn.Tanh(),
        nn.Linear(dimension // 512, 1)
    )

    optimizer = torch.optim.SGD(net.parameters(), 0.1)

    criterion = nn.BCEWithLogitsLoss()

    for e in range(10):
        out = net(Variable(torch.from_numpy(x_fit.astype('float32').to_numpy())))
        loss = criterion(out, Variable(torch.from_numpy(np.array(y_fit).astype('float'))).unsqueeze(1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (e + 1) % 2 == 0:
            print('epoch: {}, loss: {}'.format(e + 1, loss.data.item()))

    pred = F.sigmoid(net(Variable(torch.from_numpy(x_val.astype('float32').to_numpy()))))
    pred = (pred > 0.5) * 1
    num = accuracy_score(torch.from_numpy(np.array(y_val.astype('float32'))), pred)
    print("Testing accuracy {:.4f}".format(num))

    del pred, x_fit, x_val, y_fit, y_val
    gc.collect()

    pred = F.sigmoid(net(Variable(torch.from_numpy(test[test_cols].astype('float32').to_numpy()))))
    test_result = pred[:, 0].tolist()

    del test
    gc.collect()

    result = pd.read_csv('../Source/test_sample.csv')
    result['HasDetections'] = test_result
    result.to_csv('../Source/test_sample_neural_test.csv', index=False)

    print('end')


if __name__ == '__main__':
    # test function
    # neural_network_test()

    neural_network()
